[
  {
    "path": "posts/2021-12-15-session1-3/",
    "title": "Overfitting, Bias-Variance, and Cross-Validation",
    "description": "Choosing models with the right complexity.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2024-10-25",
    "categories": [],
    "contents": "\nEven if we can effectively draw curves through data, we might not actually be\nable to make good predictions on future samples. For example, suppose we fit\nthis complicated curve to a training dataset,\n\n\n\nIt would have perfect performance on the training data. Now imagine\nthat we collected a few more samples, all from the same population,\n\n\n\nIt turns out that the error is higher than a smoother curve that has higher\ntraining error,\n\n\n\nNotice that the blue test errors are all shorter here than in the previous fit,\n\n\n\nThis issue can also occur in classification. For example, the decision\nboundary here,\n\n\n\nis perfect on the training dataset. But it will likely perform worse than the\nsimpler boundary here,\n\n\n\nThis phenomenon is called overfitting. Complex models might appear to do\nwell on a dataset available for training, only to fail when they are released\n“in the wild” to be applied to new samples. We need to be very deliberate about\nchoosing models that are complex enough to fit the essential structure in a\ndataset, but not so complex that they overfit to patterns that will not appear\nagain in future samples.\nThe simplest way to choose a model with the right level of complexity is to\nuse data splitting. Randomly split the data that are available into a train and\na test sets. Fit a collection of models, all of different complexities, on the\ntraining set. Look at the performances of those models on the test set. Choose\nthe model with the best performance on the test set.\n\n\n\nA more sophisticated approach is to use cross-validation. Instead of using\nonly a single train / test split, we can group the data into \\(K\\)-different\n“folds.” For each complexity level , we can train \\(K\\) models, each time leaving\nout one of the folds. Performance of these models on the hold-out folds is used\nfor estimating the appropriate complexity level.\n\n\n\nThere are a few subtleties related to train / test splits and\ncross-validation that are worth knowing,\nWe want the held-out data to be as representative of new, real-world data as\npossible. Sometimes, randomly split data will not be representative. For\nexample, maybe the data are collected over time. A more “honest” split would\nsplit data into past vs. future samples starting at a few different\ntimepoints, training on past and evaluating on future. The sample issue occurs\nif we plan on applying a model to a new geographic context or market segment,\nfor example.\nCross-validation can be impractical on larger datasets. For this reason, we\noften see only a single train / test split for evaluation of algorithms in\nlarge data settings.\nWe incur a bias when splitting the data. The fact that we split the data\nmeans that we aren’t using all the available data, which leads to slightly\nlower performance. If it hurts more complex models more than simpler ones,\nthan it can even lead to slightly incorrect selection. After having chosen a\ngiven model complexity, it’s worth retraining that model on the full available\ndataset.\n\nLet’s implement both simple train / test and cross-validation using\nsklearn. First, let’s just use a train / test split, without cross-validating.\nWe’ll consider the penguins dataset from the earlier lecture.\n\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\n\n# prepare data\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat679_code/0330ce6257ff077c5d4ed9f102af6be089f5c486/examples/week6/week6-4/penguins.csv\")\npenguins = penguins.dropna()\nX, y = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]], penguins[\"species\"]\n(\n  X_train, X_test, \n  y_train, y_test, \n  indices_train, indices_test \n) =  train_test_split(X, y, np.arange(len(X)), test_size=0.25)\n\nLet’s double check that the sizes of the train and test sets make sense.\n\nlen(indices_train)\n249\nlen(indices_test)\n84\n\nNow, we’ll train the model on just the training set.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier() \npenguins[\"y_hat\"] = model.predict(X)\n\n# keep track of which rows are train vs. test\npenguins[\"split\"] = \"train\"\npenguins = penguins.reset_index()\npenguins.loc[list(indices_test), \"split\"] = \"test\"\n\nFinally, we’ll visualize to see the number of errors on either split. The first\nrow are the test samples, the second row are the train samples. Notice that even\nthough prediction on the training set is perfect, there are a few errors on the\ntest set.\n\nggplot(py$penguins) +\n  geom_point(aes(bill_length_mm, bill_depth_mm, col = species)) +\n  scale_color_manual(values = c(\"#3DD9BC\", \"#6DA671\", \"#F285D5\")) +\n  labs(x = \"Bill Length\", y = \"Bill Depth\") +\n  facet_grid(split ~ y_hat)\n\n\nNext, let’s look at cross validation. There is a handy wrapper function\ncalled cross_val_score in sklearn that handles all the splitting and looping\nfor us. We just have to give it a model type and it will evaluate it on a few\nsplits of the data. The scores vector below gives the error rate on \\(K = 5\\)\nholdout folds.\n\nfrom sklearn.model_selection import cross_val_score\nmodel_class = GradientBoostingClassifier()\nscores = cross_val_score(model_class, X, y, cv=5)\nscores\narray([1.        , 0.98507463, 0.94029851, 0.90909091, 0.96969697])\n\nRelated to overfitting, there is an important phenomenon called the\nbias-variance tradeoff. To understand this trade-off, imagine being able to\nsample hundreds of datasets similar to the one that is available. On each of\nthese datasets, we can train a new model (at a given\ncomplexity level). Then, we can define,\nBias: Average the predictions from models trained across each of the imaginary\ndatasets. How far off are they from the truth, on average?\nVariance: How similar are predictions from across the different runs?\n\n\n\n\nIdeally, we would have low values for both bias and variance, since they both\ncontribute to performance on the test set. In practice, though, there is a\ntrade-off. Often, the high-powered models that tend to be closest to the truth on average\nmight be far off on any individual run (high variance). Conversely, overly simple models that are very stable\nfrom run to run might be consistently incorrect in certain regions (bias).\nModels with high variance but low bias tend to be overfit, and models with low\nvariance but high bias tend to be underfit. Models that have good test or\ncross-validation errors have found a good compromise between bias and variance.\nIn practice, a useful strategy is to,\nTry overfitting the data. This ensures that the features and / or model class are sufficiently rich. This provides a kind of upper bound on the possible performance with the current setup.\nTry regularizing the overfit model so that it also performs well on holdout\ndata.\n",
    "preview": "posts/2021-12-15-session1-3/figures/overfit-1.png",
    "last_modified": "2024-10-24T10:03:34-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-15-session1-4/",
    "title": "Model Evaluation and Visualization",
    "description": "Communicating model behavior after fitting them.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2024-10-25",
    "categories": [],
    "contents": "\nIn some cases, it might be enough to know that a model is giving the correct\npredictions most of the time, without worrying too much about how it arrived at\nthat decision. If a medical classifier worked 99.9% of the time, I wouldn’t\nquestion it – I would try to figure out what to do next in the treatment plan.\nThat said, in many situations, we might want to attribute model performance to a\nfew characteristics of the data. This is often the case when we are using a\nmodel as a proxy for understanding a little more about the (biological,\nbusiness, environmental, political, social, …) processes that generated the\ndata. In these notes, we’ll review a few types of summaries and visualizations\nthat can help with these attribution tasks.\nFor (sparse) linear and logistic regression models, we can inspect the fitted\ncoefficients \\(b\\). If we have standardized the data, then the coefficients with\nthe largest coefficients lead to the largest change in the response / class\nprobabilities, all else held equal. In particular, coefficients that are\nestimated to be 0 can be safely ignored.\n\n\n\nNote that the fitted coefficient \\(\\hat{b}_{d}\\) should be interpreted in context\nof all the variables in the model, not just the \\(d^{th}\\). This is because\n\\(\\hat{b}_{d}\\) represents the effect of variable \\(d\\), after having controlled for the rest. To convince yourself that this important, you could try\nsimulating data with two correlated predictors and seeing how the fitted\ncoefficients compare to the uncorrelated predictor case.\nFor tree-based models, there are no coefficients \\(\\hat{b}\\) to use for\ninterpretation. Instead, it’s common to consider the variable importance\nstatistic. The importance of a variable measures the deterioration in a model\nwhen that variable is removed. If the model deteriorates substantially, then\nthat variable is said to be important.\n\n\n\nLet’s see how we extract coefficients and variable importance measures in\nsklearn. First, we’ll revisit the baseball prediction task. Earlier, we never\nactually discussed of the features were actually relevant. The block below\nreruns the code we had from before.\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\n\nbaseball = pd.read_csv(\"https://github.com/krisrs1128/naamii_summer_2023/raw/main/assets/baseball.csv\")\nX, y = baseball.iloc[:, 2:], baseball[\"salary\"]\n\nmodel = linear_model.ElasticNet(alpha=1e-1, l1_ratio=0.5) # in real life, have to tune these parameters\nmodel.fit(X, y)\nElasticNet(alpha=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ElasticNet?Documentation for ElasticNetiFittedElasticNet(alpha=0.1) \nbaseball[\"y_hat\"] = model.predict(X)\nbaseball[\"y\"] = y\n\nLet’s sort the predictors by the sizes of their coefficients.\n\nranks = np.argsort(np.abs(model.coef_))\nX.columns[ranks]\nIndex(['AtBat', 'Assists', 'LeagueN', 'CWalks', 'Errors', 'CAtBat',\n       'NewLeagueN', 'RBI', 'Runs', 'HmRun', 'Years', 'CHmRun', 'CHits',\n       'DivisionW', 'Walks', 'PutOuts', 'CRuns', 'CRBI', 'Hits'],\n      dtype='object')\n\nThe block below plots the features against the truth and the predicted values, for a few of the more important features.\n\n\n\nError Analysis\nEarlier, we discussed using a test set or cross-validation to evaluate how\nwell a model performs on new data. A related, but deeper, question is to\nunderstand on which types of samples a model does better or worse. That is,\ninstead of just asking how well a model performs on average, we may want to\ncharacterize situations where a model performs well and situations where it\nperforms poorly. This can be used both to summarize the limitations of a model\nand guide future improvements.\nError Quantiles: One strategy to error analysis is to take samples from\ndifferent quantiles of the error distribution and then summarize their\nassociated features. Specifically, it’s possible to create a histogram of the\nsquared error or cross-entropy losses on test samples. Take samples in the far\nright tail – these are those with very large loss. What characteristics do they\ntend to share?\n\n\n\nError Prediction: A related approach is to use the sample-level test set\nerrors from the previous step and use them as features in a new model from a\ndifferent model class. The features that help distinguish low and high-error\nsamples can be used to identify regions of poor performance. Note that, if it’s\npossible to do better than a random baseline, then there is systematic structure\nremaining in the data that has not been used by the original model class.\nVisualizations\nAs more complex models become more common in practice, visualization has\nemerged as a key way for (a) summarizing their essential structure and (b)\nmotivating further modeling refinements. We will discuss the role of ceteris\nparibus and partial dependence profiles.\nWe will write \\(x^{d \\vert = z}\\) to denote the observation \\(x\\) with the\n\\(d^{th}\\) coordinate set to \\(z\\). How should we describe the effect of changing\nthe \\(d^{th}\\) input on predictions made by complex models \\(f\\left(x\\right)\\)? As a\nthought experiment, consider the example below.\n\n\n\nThe surface is the fitted function \\(f(x)\\), mapping a two dimensional input \\(x\\)\nto a continuous response. How would you summarize the relationship between \\(x_1\\)\nand \\(y\\)? The main problem is that the shape of the relationship depends on which\nvalue of \\(x_2\\) we start at.\nOne idea is to consider the values of \\(x_2\\) that were observed in our\ndataset. Then, we can evaluate our model over a range of values \\(x_1\\) after\nfixing those values of \\(x_2\\). These curves are called Ceteris Paribus (CP)\nprofiles The same principle holds in higher dimensions. We can fix \\(D−1\\)\ncoordinates of an observation and then evaluate what happens to a sample’s\npredictions when we vary coordinate \\(d\\). Mathematically, this is expressed by\n\\(h_{x}^{f, d}\\left(z\\right) := f\\left(\\mathbf{x}^{d\\vert= z}\\right)\\)\n\n\n\n\n\n\nTo summarize a set of CP profiles, we can take their average across the\ndataset. This is called a partial dependence (PD) profile. It is a more concise\nalternative to CP profiles, showing one curve per features, rather than one\ncurve per sample.\nWe can visualize the CP profiles using the partial_dependence\nfunction in sklearn. Let’s apply it to the gradient boosting classifier we\nused for the penguins dataset. The block below refits the gradient boosting\nmodel.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport pandas as pd\n\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat679_code/0330ce6257ff077c5d4ed9f102af6be089f5c486/examples/week6/week6-4/penguins.csv\").dropna()\n\nmodel = GradientBoostingClassifier()\nX, y = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]], penguins[\"species\"]\nmodel.fit(X, y)\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier() \n\nWe now compute partial dependence scores for one of the features,\nbill_length_mm. This will give us a sense of how the three class probabilities\nchange as we vary the bill length, all else held equal.\n\nfrom sklearn.inspection import partial_dependence\npd_data = partial_dependence(model, X, [\"bill_length_mm\"], kind=\"both\")\n\nAt this point, we can visualize the CP curves. Each penguin has three lines\nbelow, one for each class. Classes are differentiated by color. As expected,\nwhen the bill length is lower, the probability of being an Adelie species is\nmuch higher – the top left has much more light green.\nI’ve hidden the code used to\nvisualize them, because it is a bit more involved than everything else covered\nso far. That said, you can read the original source on the github page.\n\n\n\nTo make the trends easier to read, we can also average the CP profiles to get\nthe partial dependence profiles. This makes the relationship between beak length\nand species type clearer. Notice that for larger beak lengths, we put about 50%\nprobability on either Chinstrap or Gentoo. This is consistent with what we’d\nobserved in the original 2D plot – those penguin species have about the same\nbeak lengths, just different beak depths. While that pattern was clear directly\nin this example, in situations with more features, this approach to visualizing\nmodel fits will help discover relationships that were unknown in advance.\n\n\n\n\n\n",
    "preview": "posts/2021-12-15-session1-4/figures/linear-1d-sparse.png",
    "last_modified": "2024-10-24T10:02:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Setting Up Supervised Learning Probems",
    "description": {},
    "author": [
      {
        "name": "Kris Sankaran",
        "url": "https://krisrs1128.github.com/LSLab"
      }
    ],
    "date": "2024-10-25",
    "categories": [],
    "contents": "\n\n\n\nWe are making predictions all the time, often without realizing it. For\nexample, imagine we are waiting at a bus stop and want to guess how long it will\nbe before a bus arrives. We can combine many sources of evidence,\nHow many people are currently at the stop? If there are more people, we think\na bus might arrive soon.\nWhat time of day is it? If it’s during rush hour, we would expect more\nfrequent service.\nWhat is the weather like? If it is poor weather, we might expect delays.\netc.\nTo think about the process formally, we could imagine a vector \\(\\mathbf{x}_i \\in \\mathbb{R}^{D}\\) reflecting \\(D\\) characteristics of our environment.\nIf we collected data about how long we actually had to wait, call it \\(y_i\\), for\nevery day in a year, then we would have a dataset\n\\[\\begin{align*}\n\\left(\\mathbf{x}_1, y_1\\right) \\\\\n\\left(\\mathbf{x}_2, y_2\\right) \\\\\n\\vdots \\\\\n\\left(\\mathbf{x}_{365}, y_{365}\\right) \\\\\n\\end{align*}\\]\nand we could try to summarize the relationship \\(\\mathbf{x}_i \\to y_i\\). Methods\nfor making this process automatic, based simply on a training dataset, are\ncalled supervised learning methods.\nIn the above example, the inputs were a mix of counts (number of people at\nstop?) and categorical (weather) data types, and our response was a nonnegative\ncontinuous value. In general, we could have arbitrary data types for either\ninput or response variable. A few types of outputs are so common that they come\nwith their own names,\n\\(y_i\\) continuous \\(\\to\\) regression\n\\(y_i\\) categorical \\(\\to\\) classification\nFor example,\nTrying to determine whether a patient’s disease will be cured by a\ntreatment is a classification problem – the outcomes are either yes, they will\nbe cured, or no, they won’t.\nTrying to estimate the crop yield of a plot of\nfarmland based on a satellite image is a regression problem – it could be any\ncontinuous, nonnegative number.\nThere are in fact many other types of responses (ordinal, multiresponse,\nsurvival, functional, image-to-image, …) each which come with their own names\nand set of methods, but for our purposes, it’s enough to focus on regression and\nclassification.\nThere is a nice geometric way of thinking about supervised learning. For\nregression, think of the inputs on the \\(x\\)-axis and the response on the\n\\(y\\)-axis. Regression then becomes the problem of estimating a one-dimensional\ncurve from data.\n\n\n\nIn higher-dimensions, this becomes a surface.\n\n\n\nIf some of the inputs are categorical (e.g., poor vs. good weather), then the\nregression function is no longer a continuous curve, but we can still identify\ngroup means.\nClassification has a similar geometric interpretation, except instead of a\ncontinuous response, we have categorical labels. We can associate classes with\ncolors. If we have only one input, classification is the problem of learning\nwhich regions of the input are associated with certain colors.\n\n\n\nIn higher-dimensions, the view is analogous. We just want to find boundaries\nbetween regions with clearly distinct colors. For example, for disease\nrecurrence, blood pressure and resting heart rate might be enough to make a good\nguess about whether a patient will have recurrence or not.\n\n\n\nModel Classes\nDrawing curves and boundaries sounds simple, but is a surprisingly difficult\nproblem, especially when the number of potentially informative features \\(D\\) is\nlarge. It helps to have predefined types of curves (and boundaries) that we can\nrefer to and use to partially automate the process of supervised learning. We’ll\ncall an example of these predefined curve types a “model class.” Let’s just build some intuition about what each model class looks\nlike and how we might be able to fit it with data.\nLinear Models\nMaybe the simplest curve is a linear one,\n\\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1.\n\\end{align*}\\]\nHere, \\(b_0\\) gives the \\(y\\)-intercept and \\(b_1\\) gives the slope.\n\n\n\nWhen we have many input features, the equivalent formula is\n\\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1 + \\dots + b_{D}x_{D} := b^{T}x,\n\\end{align*}\\]\nwhere I’ve used the dot-product from linear algebra to simplify notation (after having appended a 1). This kind of model is called a linear regression model.\n\n\n\nHow do we find a \\(b\\) that fits the data well? We can try to optimize a “loss”\nfunction. This measures the quality of the fitted line. For linear regression, a\ngood choice is squared error loss,\n\\[\\begin{align*}\nL\\left(b\\right) = \\sum_{i = 1}^{N} \\left(y_i - b^{T}x_{i}\\right)^2.\n\\end{align*}\\]\nFor classification, we can imagine drawing a linear boundary. For simplicity,\nwe’ll assume we have only two classes, though a similar partition of the space\ncan be made for arbitrary numbers of classes.\n\n\n\nTo describe this, we need to define a direction \\(b\\) perpendicular to the\nboundary. We will say that whenever\n\\[\\begin{align*}\nf_{b}\\left(x\\right) = \\frac{1}{1 + \\text{exp}\\left(b^T x\\right)}\n\\end{align*}\\]\nis larger than 0.5, we’re in the red region, and whenever it’s smaller than\n0.5, we’re in the purple region. This kind of model is called a logistic regression model.\n\n\n\nWe need a loss function for logistic regression too. In theory, we could\ncontinue to use squared error loss, but we can do better by considering the fact\nthat the true response is only one of two values. To make things concrete, say\nthat \\(y_i = 1\\) whenever it is an red point, otherwise \\(y_i = 0\\). We can use\nbinary cross-entropy loss,\n\\[\\begin{align*}\n-\\left[\\sum_{i = 1}^{N} y_i \\log\\left(f_{b}\\left(x_i\\right)\\right) + \\left(1 - y_i\\right) \\log\\left(1 - f_{b}\\left(x_i\\right)\\right)\\right]\n\\end{align*}\\]\nTo understand this loss, note that each term decomposes into either the blue or\nred curve, depending on whether the \\(y_i\\) is 1 or 0.\n\n\n\nIf 1 is predicted with probability 1, then there is no loss (and conversely for\n0). The loss increases the further the predicted probability is from the true\nclass.\nLet’s fit a linear regression in code. Below, I’m loading a dataset about\ndiabetes disease progression. The response \\(y\\) is disease severity one year\nafter diagnosis, it ranges from 25 (low severity) to 246 (high severity). There\nare \\(D = 10\\) numeric predictors; below I print 4 samples corresponding to the first 5 features.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets, linear_model\n\nX, y = datasets.load_diabetes(return_X_y=True)\nX[:4, :5] # first five predictors\narray([[ 0.03807591,  0.05068012,  0.06169621,  0.02187239, -0.0442235 ],\n       [-0.00188202, -0.04464164, -0.05147406, -0.02632753, -0.00844872],\n       [ 0.08529891,  0.05068012,  0.04445121, -0.00567042, -0.04559945],\n       [-0.08906294, -0.04464164, -0.01159501, -0.03665608,  0.01219057]])\ny[:4] # example response\narray([151.,  75., 141., 206.])\n\nLet’s now fit a linear model from \\(\\mathbf{x}_1, \\dots, \\mathbf{x}_{N}\\) to \\(y\\).\nThe first line tells python that we are using a LinearRegression model class.\nThe second searches over coefficients \\(b\\) to minimize the squared-error loss\nbetween the \\(b^T x_i\\) and \\(y_i\\). The third line prints out the fitted coefficient\n\\(\\hat{b}\\).\n\nmodel = linear_model.LinearRegression()\nmodel.fit(X, y)\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LinearRegression?Documentation for LinearRegressioniFittedLinearRegression() \nmodel.coef_ # fitted b coefficients\narray([ -10.0098663 , -239.81564367,  519.84592005,  324.3846455 ,\n       -792.17563855,  476.73902101,  101.04326794,  177.06323767,\n        751.27369956,   67.62669218])\n\nLet’s do the same thing for a logistic regression. This time, we’ll use the\nPalmer’s Penguins dataset, which tries to classify penguins into one of three\ntypes based on their appearance. For example, two of the features are bill\nheight and bill depth (figure from Allison Horst’s palmerspenguins\npackage).\n\n\n\nWe’ll read the data from a public link and print the first few rows.\n\nimport pandas as pd\n\npenguins = pd.read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat679_code/0330ce6257ff077c5d4ed9f102af6be089f5c486/examples/week6/week6-4/penguins.csv\")\npenguins.head()\n  species     island  bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g     sex  year\n0  Adelie  Torgersen            39.1           18.7                181         3750    male  2007\n1  Adelie  Torgersen            39.5           17.4                186         3800  female  2007\n2  Adelie  Torgersen            40.3           18.0                195         3250  female  2007\n3  Adelie  Torgersen            36.7           19.3                193         3450  female  2007\n4  Adelie  Torgersen            39.3           20.6                190         3650    male  2007\n\nWe’ll predict species using just bill length and depth. First, let’s make a\nplot to see how easy / difficult it will be to create a decision boundary.\n\nggplot(py$penguins) +\n  geom_point(aes(bill_length_mm, bill_depth_mm, col = species)) +\n  scale_color_manual(values = c(\"#3DD9BC\", \"#6DA671\", \"#F285D5\")) +\n  labs(x = \"Bill Length\", y = \"Bill Depth\")\n\n\nIt seems like we should be able to draw nice boundaries between these\nclasses. Let’s fit the model.\n\nmodel = linear_model.LogisticRegression()\npenguins = penguins.dropna()\nX, y = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]], penguins[\"species\"]\nmodel.fit(X, y)\nLogisticRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  LogisticRegression?Documentation for LogisticRegressioniFittedLogisticRegression() \npenguins[\"y_hat\"] = model.predict(X)\n\nThe plot below compares the predicted class (left, middle, and right panels)\nwith the true class (color). We get most of the samples correct, but have a few\nmissclassifications near the boundaries.\n\nggplot(py$penguins) +\n  geom_point(aes(bill_length_mm, bill_depth_mm, col = species)) +\n  scale_color_manual(values = c(\"#3DD9BC\", \"#6DA671\", \"#F285D5\")) +\n  labs(x = \"Bill Length\", y = \"Bill Depth\") +\n  facet_wrap(~ y_hat)\n\n\nExercise: Repeat this classification, but using at least two additional\npredictors.\nSparse Linear Models\nIn many cases, we will have recorded many types of features – coordinates\nof \\(x_{i}\\) – that are not actually related to the response. A model that knows\nto ignore irrelevant features will do better than a model that tries to use\nall of them. This is the main idea behind using sparsity in linear regression.\nWe again fit the model\n\\[\\begin{align*}\nf_{b}\\left(x\\right) = b_0 + b_1 x_1 + \\dots + b_{D}x_{D} := b^{T}x,\n\\end{align*}\\]\nbut we make the assumption that many of the \\(b_{d}\\) are exactly 0. Graphically,\nwe imagine that the response does not change at all as we change some of the\ninputs, all else held equal.\n\n\n\nThe same idea can be applied to logistic regression. In this case, having a\ncoefficient \\(b_d = 0\\) means that the probabilities for different class labels\ndo not change at all as features \\(x_d\\) is changed.\n\n\n\nTo implement sparse linear regression using sklearn, we can use the\nElasticNet class. We’ll work with a dataset of American Baseball sports\nstatistics. The task is to predict each player’s salary based on their batting\nstatistics.\n\nimport pandas as pd\n\n\nbaseball = pd.read_csv(\"https://github.com/krisrs1128/naamii_summer_2023/raw/main/assets/baseball.csv\")\nbaseball.head()\n              player    salary     AtBat      Hits     HmRun      Runs       RBI     Walks     Years  ...     CRuns      CRBI    CWalks   LeagueN  DivisionW   PutOuts   Assists    Errors  NewLeagueN\n0        -Alan Ashby -0.135055 -0.601753 -0.594542 -0.527545 -1.203816 -0.521069 -0.097342  1.395233  ... -0.121439  0.258473  0.434506  1.056743   0.979299  1.219174 -0.522196  0.212946    1.073007\n1       -Alvin Davis -0.123972  0.511566  0.491323  0.728577  0.440675  0.792549  1.606310 -0.899485  ... -0.414315 -0.199211  0.010353 -0.942706   0.979299  2.105095 -0.253380  0.818404   -0.928417\n2      -Andre Dawson -0.079637  0.626971  0.735088  0.956963  0.401520  1.024364 -0.189431  0.769401  ...  1.409364  1.569674  0.354977  1.056743  -1.017256 -0.324044 -0.742763 -0.846605    1.073007\n3  -Andres Galarraga -0.985164 -0.561022 -0.461579 -0.184967 -0.616498 -0.366526 -0.511743 -1.108096  ... -0.945718 -0.879551 -0.860675  1.056743  -1.017256  1.837176 -0.542874 -0.695240    1.073007\n4   -Alfredo Griffin  0.474541  1.292248  1.355583 -0.870124  0.753911 -0.018804 -0.281520  0.769401  ...  0.422041  0.017261 -0.250955 -0.942706   0.979299 -0.031118  2.083253  2.483412   -0.928417\n\n[5 rows x 21 columns]\nX, y = baseball.iloc[:, 2:], baseball[\"salary\"]\ny = (y - y.mean()) / y.std() # standardize\n\nThe block below fits the Elastic Net model and saves the coefficients \\(\\hat{b}\\).\nNotice that most of them are 0 – only a few of the features make a big difference\nin the salary.\n\nfrom sklearn import datasets, linear_model\nmodel = linear_model.ElasticNet(alpha=1e-1, l1_ratio=0.5) # in real life, have to tune these parameters\nmodel.fit(X, y)\nElasticNet(alpha=0.1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  ElasticNet?Documentation for ElasticNetiFittedElasticNet(alpha=0.1) \ny_hat = model.predict(X)\n\nbeta_hat = model.coef_ # notice the sparsity\nbeta_hat\narray([ 0.        ,  0.17423133,  0.        ,  0.        ,  0.        ,\n        0.1050172 ,  0.        ,  0.        ,  0.07963389,  0.06285448,\n        0.12726386,  0.16911053,  0.        ,  0.        , -0.0988662 ,\n        0.12489593, -0.        , -0.        ,  0.        ])\n\nWe can confirm that the predictions are correlated relatively well with the\ntruth.\n\n            [,1]         [,2]\n0   -0.135055108 -0.016427954\n1   -0.123971550  0.296625967\n2   -0.079637319  0.836969714\n3   -0.985163996 -0.193317250\n4    0.474540574  0.174597235\n5   -1.032823295 -0.797066881\n6   -0.966321948 -0.876866640\n7   -1.021739737 -0.834469088\n8    1.250389625  0.695783793\n9   -0.041636232  0.669700610\n10  -0.051928424 -0.584211792\n11   0.031198260 -0.074863961\n12   0.363704996 -0.467194917\n13  -0.655982327 -0.325733310\n14   0.529958364  1.050398025\n15  -0.800068580 -0.945921557\n16  -0.888737043 -0.904460693\n17  -0.966321948 -0.226783826\n18  -0.933071274 -0.353904469\n19   0.142033839  0.077451983\n20   0.533653622  1.596657954\n21   0.507791248  0.442089669\n22   0.382176853 -0.371692791\n23   0.474540574  0.109540535\n24   0.197451628  0.132038662\n25   0.807047310  0.762115250\n26  -0.944154832 -0.591503678\n27   0.169742733  0.152641555\n28  -0.522979633 -0.684838193\n29   0.696211732  0.387347327\n30  -0.988489063 -0.751894525\n31  -1.038365074 -0.602162962\n32  -0.788985022 -0.622521292\n33  -0.511896075 -0.420695837\n34  -0.711400117 -0.434539235\n35  -0.639356991 -0.659088703\n36   0.618626827  0.270983238\n37   0.751629521  0.703274377\n38  -1.032823295 -0.695245805\n39   1.472060782  0.357228560\n40   0.308287206  0.559825936\n41  -0.268057802 -0.571795489\n42  -0.434311170 -0.603384097\n43  -0.264362544 -0.417730892\n44   1.804567518  0.697128086\n45  -0.988489063 -0.402694446\n46  -0.578397422  0.058097894\n47  -0.678149443 -0.857771950\n48  -0.689233001 -0.837235722\n49   0.917882889  1.278327372\n50  -1.021739737 -0.857251436\n51  -0.955238390 -0.150018032\n52  -0.478645402  0.398717839\n53   0.696211732  0.555832278\n54  -0.002052414  1.732530758\n55   0.880936957  1.379264455\n56   0.696211732  0.012256994\n57  -0.722483675 -0.442549371\n58  -0.467561844 -0.245293390\n59  -0.578397422 -0.618869284\n60  -0.190472897  0.434753072\n61   3.190012251  1.197509140\n62   3.023758883  0.948470654\n63   0.142033839  0.143880088\n64   1.121082189  1.156646047\n65  -0.944154832 -0.394585753\n66  -0.611648096  0.053795965\n67  -0.135055108 -0.329084300\n68  -0.231482061 -0.445028879\n69   1.516395013  0.258387027\n70  -1.032823295 -0.574875196\n71  -0.866569927 -0.264577547\n72   0.130950281 -0.266703984\n73   2.938326819  1.547107247\n74  -0.522979633 -0.220655904\n75  -0.101804434 -0.250246536\n76   4.265117363  1.615934897\n77  -0.356726265 -0.601579785\n78   0.474540574  0.591150888\n79   1.416642993  0.411463136\n80  -1.032823295 -0.769640826\n81   2.137074254  1.007630993\n82  -0.334559149 -0.211791319\n83   3.080442414  1.329957774\n84  -0.711400117  0.380273450\n85   0.807047310  0.189085139\n86  -0.844402811 -0.477826058\n87   0.363704996  0.534639550\n88  -0.002052414  0.020202881\n89  -0.384435160 -0.358246690\n90   0.437594643  0.953867405\n91  -0.744650790  0.911074305\n92  -0.301308476 -0.003155397\n93  -0.301308476 -0.202226671\n94   0.446831680  0.222218624\n95  -0.079637319 -0.215628600\n96   0.142033839 -0.005313397\n97   0.280578312  0.252848694\n98   0.917882889  0.409401995\n99   0.474540574 -0.062874121\n100 -0.528521412 -0.513259342\n101 -0.467561844  0.391593900\n102 -0.994030842 -0.587932764\n103 -0.800068580 -0.897992535\n104 -0.988489063 -0.750433701\n105  1.555187466  0.571637575\n106 -0.234807129  0.231221459\n107 -0.966321948 -0.374118589\n108 -0.822235695 -0.125199889\n109 -0.633815212  0.226944129\n110  1.693731939  0.546180593\n111  0.526263106  0.887313809\n112  1.047190325  0.467034114\n113 -0.578397422 -0.459353681\n114  0.529958364  0.272610745\n115  0.696211732  0.215312541\n116 -0.378893381 -0.028228342\n117 -0.977405505 -0.979336753\n118 -0.944154832 -0.653596946\n119 -0.966321948 -0.230260452\n120 -0.572855644  0.201813769\n121 -1.010656179 -0.559679803\n122  0.142033839  0.179625898\n123 -0.744650790 -0.177481921\n124  0.268386398  0.429125602\n125 -1.021739737 -0.899774381\n126  4.159823563  1.754158746\n127 -0.633815212 -0.332556571\n128 -0.844402811 -0.348831459\n129  0.230702301  0.107292550\n130 -0.522979633 -0.350526850\n131 -0.944154832 -0.777912253\n132  0.640793942  0.398105679\n133 -0.755734348 -0.392992938\n134 -0.190472897 -0.477858654\n135  0.208535186  0.007914549\n136 -0.996247554 -0.888539537\n137  1.693731939  0.346166488\n138  1.028718467  0.759605161\n139  2.802087725  1.719124925\n140  1.715899055  0.752489936\n141  0.446831680 -0.210149244\n142  0.197451628  0.126914469\n143 -0.910904158 -0.458586116\n144  1.124775230  0.413923478\n145  0.419122785  0.110214550\n146 -0.522979633 -0.326767407\n147 -0.378893381  0.226564077\n148 -1.021739737 -0.821647108\n149  1.435114850  0.865461273\n150 -0.739109011 -0.092179571\n151 -0.689233001  0.061523440\n152 -0.024219529  0.410178473\n153 -0.600564538 -0.735703112\n154  0.557667258  0.588199714\n155  0.585376153  0.513901165\n156  0.114324944  0.525620107\n157 -0.866569927 -0.498133172\n158 -0.256974244  0.720892601\n159 -1.021739737 -0.753816535\n160  0.086616049  0.278980341\n161  0.541041922 -0.118742768\n162 -0.988489063 -0.654893015\n163 -0.855486369 -0.592506556\n164  0.363704996  0.347867774\n165  0.031198260 -0.056522669\n166  0.252869417 -0.181642888\n167 -1.037256718 -0.957042699\n168 -0.966321948 -0.402379721\n169  0.297203649 -0.422854470\n170 -0.800068580 -0.109056085\n171 -0.884303619 -0.712654874\n172  3.527690574 -1.050275058\n173  0.751629521 -0.037630346\n174 -0.921987716 -0.657357598\n175 -0.877653485 -0.517336462\n176 -0.722483675  0.105041226\n177  0.585376153  0.020201816\n178 -0.655982327 -0.637189563\n179 -0.412144055 -0.191954382\n180 -0.800068580 -0.464498208\n181 -0.744650790 -0.078477727\n182  3.112427345  0.515793318\n183  0.363704996 -0.113110903\n184  0.474540574  0.086570512\n185 -0.190472897  0.127541161\n186 -0.806718714 -0.320633525\n187  1.605063476  0.279290388\n188  0.474540574  1.448551043\n189 -0.766817906 -0.515106282\n190  0.097699607  0.334307227\n191 -0.899820600 -0.599055488\n192 -0.190472897 -0.137286161\n193 -0.522979633 -0.417226652\n194 -0.633815212 -0.258565805\n195  1.139554046  0.829743869\n196 -0.711400117 -0.057499484\n197 -0.301308476  0.218664917\n198  0.053365376  0.044742258\n199  2.513915221  0.913930624\n200 -0.107346213  1.714745102\n201 -0.245890687 -0.423200359\n202 -0.079637319  0.299072479\n203 -0.633815212 -0.594283080\n204 -0.301308476 -0.060252928\n205 -0.190472897 -0.150910103\n206  0.474540574  0.074660922\n207 -1.032823295 -0.646880202\n208  0.751629521 -0.242195973\n209 -0.766817906 -0.863081047\n210 -0.764601195 -0.314220044\n211  0.452373459  0.487669956\n212 -0.633815212 -0.392492992\n213 -0.877653485 -0.671898927\n214 -0.971863727 -0.636731706\n215  0.452373459  0.376023696\n216 -0.877653485 -0.326483685\n217 -0.430615912 -0.546594799\n218  1.028718467  1.186968530\n219 -0.966321948  0.266004024\n220 -0.988489063 -0.811629230\n221 -0.744650790  0.452791831\n222 -0.888737043 -0.537291545\n223 -0.844402811 -0.165805103\n224 -0.135055108 -0.143870798\n225  2.026238675  1.523274774\n226 -0.855486369 -0.236037416\n227 -0.955238390 -0.451023699\n228 -0.412144055 -0.267907427\n229 -0.988489063  0.368251838\n230 -0.013135971  0.469596706\n231 -0.430615912 -0.261287936\n232  0.895715773  0.217359619\n233 -0.412144055  0.200064477\n234 -0.463866586 -0.311167733\n235 -0.633815212 -0.537374618\n236  0.452373459  0.292336841\n237 -0.245890687 -0.647867634\n238  0.862465100  0.420804149\n239 -0.777901464 -0.571548148\n240  0.851381542 -0.962313080\n241 -0.552535049 -0.648154649\n242 -0.644898770 -0.678178645\n243 -0.667065885 -0.317054653\n244  1.361225203  0.532519723\n245 -0.833319253 -0.150303963\n246 -0.245890687 -0.193212904\n247  0.807047310 -0.434045213\n248 -0.079637319  0.548961007\n249 -0.572855644 -0.403167896\n250  0.474540574  0.110636811\n251 -0.833319253  0.015171595\n252  1.693731939  0.995800185\n253 -0.024219529 -0.187536583\n254  0.031198260 -0.125064409\n255  2.358745411  0.758562714\n256 -0.921987716 -0.188908725\n257 -0.822235695  0.311761865\n258  0.363704996  0.148495209\n259  0.751629521  0.817735927\n260 -0.334559149 -0.349916932\n261  0.940050005  1.001580950\n262  1.028718467  0.379816267\n\n\nbaseball <- data.frame(py$X, y = py$y, y_hat = py$y_hat)\nggplot(baseball) +\n  geom_point(aes(y, y_hat)) +\n  geom_abline(slope = 1, col = \"red\") +\n  labs(x = \"True Salary\", y = \"Predicted Salary\")\n\n\nTree-based Models\nTree-based models fit a different class of curves. To motivate them,\nconsider making a prediction for the bus time arrival problem using the\nfollowing diagram,\n\n\n\nNotice that we can use the same logic to do either regression or classification.\nFor regression, we associate each “leaf” at the bottom of the tree with a\ncontinuous prediction. For classification, we associate leaves with\nprobabilities for different classes. It turns out that we can train these models\nusing squared error and cross-entropy losses as before, though the details are\nbeyond the scope of these notes.\nIt’s not immediately obvious, but these rules are equivalent to drawing\ncurves that are piecewise constant over subsets of the input space. Let’s\nconvince ourselves using some pictures. First, notice that a tree with a single\nsplit is exactly a “curve” that takes on two values, depending on the split\npoint,\n\n\n\nIf we split the same variable deeper, it creates more steps,\nWhat if we had two variables? Depending on the order, of the splits, we create\ndifferent axis-aligned partitions,\n\n\n\nQ: What would be the diagram if I had switched the order of the splits (traffic before rain)?\nA very common variation on tree-based models computes a large ensemble of\ntrees and then combines their curves in some way. How exactly they are combined\nis beyond the scope of these notes, but this is what random forests and gradient\nboosted decision trees are doing in the background.\nWe can implement these models in sklearn using RandomForestRegressor /\nRandomForestClassifier, and GradientBoostingRegressor /\nGradientBoostingClassifier. Let’s just see an example of a boosting classifier\nusing the penguins dataset. The fitting / prediction code is very similar to what we used for the sparse regression.\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier()\nX, y = penguins[[\"bill_length_mm\", \"bill_depth_mm\"]], penguins[\"species\"]\nmodel.fit(X, y)\nGradientBoostingClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFittedGradientBoostingClassifier() \npenguins[\"y_hat\"] = model.predict(X)\n\nWe use the same visualization code to check predictions against the truth. The\nboosting classifier makes no mistakes on the training data.\n\nggplot(py$penguins) +\n  geom_point(aes(bill_length_mm, bill_depth_mm, col = species)) +\n  scale_color_manual(values = c(\"#3DD9BC\", \"#6DA671\", \"#F285D5\")) +\n  labs(x = \"Bill Length\", y = \"Bill Depth\") +\n  facet_wrap(~ y_hat)\n\n\nRelationships across classes\nThe diagram below summarizes the relationships across elements of the model\nclass.\n\n\n\nWhen should we use which of these approaches? Here are some relative\nstrengths and weaknesses.\n\nStrengths\nWeaknesses\nLinear / Logistic Regression\n* Often easy to interpret* No tuning parameters* Very fast to train\n* Unstable when many features to pick from* Can only fit linear curves / boundaries (though, see featurization notes)\nSparse Linear / Logistic Regression\n* Often easy to interpret* Stable even when many features to pick from* Very fast to train\n* Can only fit linear curves / boundaries\nTree-based Classification / Regression\n* Can fit nonlinear functions of inputs\n* Can be slow to train* Somewhat harder to interpret\nTry matching models to responses in the examples below,\nQ1: We want to predict whether a patient has a disease given just their genetic\nprofile. There are 1000 genes that can serve as predictors. There are only two\npossible responses.\nQ2: A user on a site has been watching (too many…) episodes of Doctor\nWho. How many more minutes will they remain on the site today? As predictors,\nyou have features of their current and past viewing behavior (e.g.,\ncurrent time of day, number of hours on the site per week for each of the last 4 weeks,\netc.). We suspect that there are important nonlinear relationships between\nthese predictors and the response.\nQ3: We are trying to predict the next hour’s total energy production in a wind\nfarm. We have a years worth of past production and weather data, but right now,\nwe just want a baseline using current wind speed and the last hour’s production.\nThe answers are,\nA1: Sparse logistic regression. We have two classes, and most of the genes are unlikely to be relevant for classification.\nA2: A tree-based method, like random forests or gradient boosting. This is because we anticipate a nonlinear relationship.\nA3: Linear regression. The response is continuous and we just need a baseline\nusing two predictors.\n",
    "preview": "posts/welcome/figures/curve-1d.png",
    "last_modified": "2024-10-24T10:05:50-05:00",
    "input_file": {}
  }
]
